{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXmz1IBRtXNlOxyaXSgLwj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elwing-Chou/tibame20251203/blob/main/tibame20260112.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vEmLF6THyxtR"
      },
      "outputs": [],
      "source": [
        "# 成品1\n",
        "import urllib.request as req\n",
        "import bs4 as bs\n",
        "import os\n",
        "\n",
        "# M.....html/meta.json\n",
        "def get_page_data(url, dirname=\".\"):\n",
        "    # url = \"https://www.ptt.cc/bbs/Beauty/M.1766978298.A.90C.html\"\n",
        "    resp = req.urlopen(url)\n",
        "    content = resp.read()\n",
        "    html = bs.BeautifulSoup(content)\n",
        "    # print(html)\n",
        "\n",
        "    uid_text = \"-\"\n",
        "    board_text = \"-\"\n",
        "    title_text = \"-\"\n",
        "    date_time_text = \"-\"\n",
        "    # \"uid\" \"type\":-1/0/1(需/->/推) \"text\"  \"ipfatetime\"\n",
        "    pushes = []\n",
        "\n",
        "    metas = html.find_all(\"span\", {\"class\":\"article-meta-value\"})\n",
        "    if len(metas) == 4:\n",
        "        uid, board, title, date_time = metas\n",
        "        uid_text = uid.get_text().strip()\n",
        "        board_text = board.get_text().strip()\n",
        "        title_text = title.get_text().strip()\n",
        "        date_time_text = date_time.get_text().strip()\n",
        "\n",
        "    pushes_meta = html.find_all(\"div\", {\"class\":\"push\"})\n",
        "    for p in pushes_meta:\n",
        "        p_tag, p_uid, p_text, p_ipdatetime = p.find_all(\"span\")\n",
        "        p_tag_text = p_tag.get_text().strip()\n",
        "        p_uid_text = p_uid.get_text().strip()\n",
        "        p_text_text = p_text.get_text().strip().replace(\": \", \"\", 1)\n",
        "        p_ipdatetime_text = p_ipdatetime.get_text().strip()\n",
        "\n",
        "        p_tag_conv = {\n",
        "            \"推\":1,\n",
        "            \"→\":0,\n",
        "            \"噓\":-1\n",
        "        }\n",
        "        p_tag_text = p_tag_conv[p_tag_text]\n",
        "\n",
        "        p_dict = {\n",
        "            \"tag\":p_tag_text,\n",
        "            \"uid\":p_uid_text,\n",
        "            \"text\":p_text_text,\n",
        "            \"ipdatetime\":p_ipdatetime_text\n",
        "        }\n",
        "        pushes.append(p_dict)\n",
        "\n",
        "    # extract\n",
        "    main_content = html.find(\"div\", {\"id\":\"main-content\"})\n",
        "    metas = main_content.find_all(\"div\", {\"class\":\"article-metaline\"})\n",
        "    for m in metas:\n",
        "        m.extract()\n",
        "    metas = main_content.find_all(\"div\", {\"class\":\"article-metaline-right\"})\n",
        "    for m in metas:\n",
        "        m.extract()\n",
        "    metas = main_content.find_all(\"div\", {\"class\":\"push\"})\n",
        "    for m in metas:\n",
        "        m.extract()\n",
        "    metas = main_content.find_all(\"span\", {\"class\":\"f2\"})\n",
        "    for m in metas:\n",
        "        m_text = m.get_text()\n",
        "        if \"發信站:\" in m_text:\n",
        "            m.extract()\n",
        "        if \"文章網址:\" in m_text:\n",
        "            m.extract()\n",
        "\n",
        "    main_content_text = main_content.get_text()\n",
        "\n",
        "    data = {\n",
        "        \"uid\":uid_text,\n",
        "        \"board\":board_text,\n",
        "        \"title\":title_text,\n",
        "        \"datetime\":date_time_text,\n",
        "        \"content\":main_content_text,\n",
        "        \"pushes\":pushes\n",
        "    }\n",
        "\n",
        "    if not os.path.exists(dirname):\n",
        "        os.makedirs(dirname)\n",
        "\n",
        "    fp = dirname + \"/meta.json\"\n",
        "    f = open(fp, \"w\", encoding=\"utf-8\")\n",
        "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "    f.close()\n",
        "\n",
        "    return data\n",
        "\n",
        "# 成品2\n",
        "# .: 這層 ..: 上一層\n",
        "def download_img(url, dirname=\".\"):\n",
        "    if not os.path.exists(dirname):\n",
        "        os.makedirs(dirname)\n",
        "    # url = \"https://i.imgur.com/opGiGRV.jpeg\"\n",
        "    h = {\n",
        "        \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36\"\n",
        "    }\n",
        "    r = req.Request(url, headers=h)\n",
        "    resp = req.urlopen(r)\n",
        "    content = resp.read()\n",
        "    # 1. 純文字: txt 2. 非純文字\n",
        "    # r/w+encoding      rb/wb\n",
        "    fp = dirname + \"/\" + url.split(\"/\")[-1]\n",
        "    # print(fp)\n",
        "    f = open(fp, \"wb\")\n",
        "    f.write(content)\n",
        "    f.close()\n",
        "\n",
        "# demo\n",
        "# download_img(\"https://i.imgur.com/opGiGRV.jpeg\", \"aa\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_page_ims(url):\n",
        "    # url = \"https://www.ptt.cc/bbs/Beauty/M.1767140595.A.150.html\"\n",
        "    resp = req.urlopen(url)\n",
        "    content = resp.read()\n",
        "    html = bs.BeautifulSoup(content)\n",
        "\n",
        "    save_dir = url.split(\"/\")[-1]\n",
        "    get_page_data(url, save_dir)\n",
        "\n",
        "    links = html.find_all(\"a\")\n",
        "    allow_subnames = {\"jpg\", \"png\", \"gif\", \"jpeg\", \"mp4\"}\n",
        "    for l in links:\n",
        "        href = l[\"href\"]\n",
        "        sub = href.split(\".\")[-1]\n",
        "        if sub.lower() in allow_subnames:\n",
        "            print(href)\n",
        "            download_img(href, save_dir)"
      ],
      "metadata": {
        "id": "PJeIVbcQ0nMz"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}